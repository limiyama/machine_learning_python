# ctrl c + ctrl v dos rascunhos da tarefa só para não perder
import warnings
warnings.filterwarnings("ignore")

import os
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_curve, auc


np.random.seed(0)   # Do not change this value: required to be compatible with solutions generated by the autograder.

train_df = pd.read_csv("assets/train.csv")
print(train_df.shape)
train_df.head()

# gráfico p ver features mais importantes
X = train_df.iloc[:,1:-1]
y = train_df.iloc[:,-1]

selector = SelectKBest(f_classif, k='all')
selector.fit(X, y)
scores = selector.scores_

feature_scores = pd.DataFrame({'Feature': X.columns, 'Score': scores})
total = feature_scores['Score'].sum()
feature_scores['Score'] = feature_scores['Score']/total
feature_scores.sort_values('Score', ascending=False, inplace=True)

plt.figure(figsize=(6,3))
sns.barplot(x='Score', y='Feature', data=feature_scores)
plt.xlabel('Score')
plt.ylabel('Features')

plt.show()

# features de maior importância segundo o gráfico
features = ['document_entropy', 'freshness']
X = train_df[features]
y = train_df.iloc[:, -1]
y = y.astype(int)
X.head()


# usando as duas features
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

model = GaussianNB().fit(X_train, y_train)
y_pred = model.predict(X_test)
    
test_features = test_df[features]
probabilities = model.predict_proba(test_features)[:, 1] 
    
return pd.Series(probabilities, index=test_df['id'])
